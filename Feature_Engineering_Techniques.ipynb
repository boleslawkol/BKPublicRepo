{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boleslawkol/Purdue-Notebooks/blob/main/Feature_Engineering_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common issues\n",
        "1. Missing data\n",
        "2. Categorical variables\n",
        "3. Imbalanced data\n",
        "4. Linear assumptions\n",
        "5. Distributions\n",
        "6. Outliers\n",
        "7. Feature scale"
      ],
      "metadata": {
        "id": "aoKiYaho0eQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imputing missing values\n",
        "\n",
        "### Numerical\n",
        "- Mean/Median imputation\n",
        "- Arbitrary\n",
        "- End of tail\n",
        "\n",
        "### Categorical\n",
        "- Mode\n",
        "- Add \"Missing\" category\n",
        "\n",
        "### Numerical and Categorical\n",
        "- Complete Case Analysis\n",
        "- Add a missing indicator\n",
        "- Random sample imputation"
      ],
      "metadata": {
        "id": "ETp9AlOO2Dnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before applying feature engineering, split yout dataset in train  and test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=.25,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "t0v6E9Ms8N-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LTESUTCyi5v"
      },
      "outputs": [],
      "source": [
        "# Numerical - Mean/Median Imputation\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# for normal distributions\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# for skewed distributions\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Fit and transform\n",
        "imputer.fit(X_train)\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assumptions\n",
        "- Data is missing at random\n",
        "- Missing data would look like most of your observations\n",
        "\n",
        "#### Pros\n",
        "- Easy to implement\n",
        "- Fast\n",
        "- Can be used in production\n",
        "\n",
        "#### Cons\n",
        "- Distorts: distributions, variance and covariance\n",
        "- The more missing values the higher the distortion"
      ],
      "metadata": {
        "id": "MpWkg_7p3ead"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical - Arbitrary Imputation\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# for normal distributions\n",
        "imputer = SimpleImputer(strategy='constant', fill_value=999)\n",
        "\n",
        "# Fit and transform\n",
        "imputer.fit(X_train)\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ],
      "metadata": {
        "id": "H7F6K25r3blb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assumptions\n",
        "- Data is missing at random\n",
        "\n",
        "#### Pros\n",
        "- Easy to implement\n",
        "- Fast\n",
        "- Can be used in production\n",
        "- Captures the importance of a value being missing\n",
        "\n",
        "#### Cons\n",
        "- Distorts: distributions, variance and covariance\n",
        "- The more missing values the higher the distortion\n",
        "- It can mask or create outliers\n",
        "- Be careful not to use values that are too similar to mean/median"
      ],
      "metadata": {
        "id": "94R2gE9O0ca2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical - End ot tail imputation\n",
        "\n",
        "from feature_engine.imputation import EndTailImputer\n",
        "\n",
        "# for normal distributions\n",
        "imputer = EndTailImputer(imputation_method='gaussian', tail='both')\n",
        "\n",
        "# for skewed distributions\n",
        "imputer = EndTailImputer(imputation_method='iqr', tail='both')\n",
        "\n",
        "# Fit and transform\n",
        "imputer.fit(X_train)\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ],
      "metadata": {
        "id": "tTuXsjTP6Sci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assumptions\n",
        "- Data is missing at random\n",
        "\n",
        "#### Pros\n",
        "- Easy to implement\n",
        "- Fast\n",
        "- Can be used in production\n",
        "\n",
        "#### Cons\n",
        "- Distorts: distributions, variance and covariance\n",
        "- The more missing values the higher the distortion"
      ],
      "metadata": {
        "id": "i4HV6Qs47uYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical - Frequency/Mode imputation\n",
        "\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Fit and transform\n",
        "imputer.fit(X_train)\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ],
      "metadata": {
        "id": "G4muSUq472Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assumptions\n",
        "- Data is missing at random\n",
        "- Missing observations most likely look like the majority\n",
        "\n",
        "#### Pros\n",
        "- Easy to implement\n",
        "- Fast\n",
        "- Can be used in production\n",
        "\n",
        "#### Cons\n",
        "- Distorts in the relation between the most frequent values and other variables\n",
        "- Overrepresentation of the mode if there are many missing values"
      ],
      "metadata": {
        "id": "9rKazhXA-Z0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical - \"Missing\" new category\n",
        "\n",
        "imputer = SimpleImputer(strategy='constant', fill_value=\"Missing\")\n",
        "\n",
        "# Fit and transform\n",
        "imputer.fit(X_train)\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ],
      "metadata": {
        "id": "u-LNbuDH_S4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pros\n",
        "- Easy to implement\n",
        "- Fast\n",
        "- Can be used in production\n",
        "- Capture the importance of missing data\n",
        "- There are not assumpiotn of data missing at random or not\n",
        "\n",
        "#### Cons\n",
        "- If the number of missing values is small you can end up with a rare category"
      ],
      "metadata": {
        "id": "lSQV9jFF_8PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical and Categorical - Complete Case Analysis\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "pLg0ieIr_6yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assumptions\n",
        "- Data is missing at random\n",
        "\n",
        "#### Pros\n",
        "- Easy to implement\n",
        "- No data manipulation required\n",
        "- Preserves distributions\n",
        "\n",
        "#### Cons\n",
        "- A lot of observations can be discarded if there is a significant amount of missing data\n",
        "- Can create a biased datset when your CCA differ from the original data\n",
        "- Can't be used in production\n",
        "\n",
        "#### When to use CCA:\n",
        "- Data is completely missing at random\n",
        "- No more than 5% of observation will be discarded"
      ],
      "metadata": {
        "id": "Q4_R70oyBHYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical and Categorical - Missing Indicator\n",
        "\n",
        "from pandas.core.internals import concat\n",
        "from sklearn.impute import MissingIndicator\n",
        "\n",
        "indicator = MissingIndicator(features='missing-only')\n",
        "\n",
        "# Fit\n",
        "indicator.fit(X_train)\n",
        "\n",
        "# Print and get columns with missing indicator\n",
        "print(X_train.columns[indicator.features_])\n",
        "temp = indicator.transform(X_train)\n",
        "\n",
        "# Create columns for each new indicator\n",
        "indicator_columns =[column + \"_NA_IND\" for column in X_train.columns[indicator.features_]]\n",
        "indicator_df = pd.DataFrame(temp, columns=indicator_columns)\n",
        "\n",
        "# Concat columns with indicators and rest of training data\n",
        "X_train= = pd.concat([X_train.reset_index(), indicator_df], axis=1)\n",
        "\n",
        "# Same for Test\n",
        "temp_test = indicator.transform(X_test)\n",
        "test_indicator_df = pd.DataFrame(temp_test, columns=indicator_columns)\n",
        "\n",
        "X_test = pd.concat([X_test.reset_index(), test_indicator_df], axis=1)"
      ],
      "metadata": {
        "id": "GV9NXmZZB9gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assumptions\n",
        "- Data is NOT misssing at random\n",
        "- Missing data can be predicted\n",
        "\n",
        "#### Pros\n",
        "- Easy to implement\n",
        "- Can be integrated in production\n",
        "- Captures the importance of missing data\n",
        "\n",
        "#### Cons\n",
        "- Expands the feature set\n",
        "- The original variable still has to be imputed\n",
        "- Many missing indicators may be very highly correlated"
      ],
      "metadata": {
        "id": "I8JrfkhsDYcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.construct import random\n",
        "# Numerical and Categorical - Random Sample Imputation\n",
        "\n",
        "from feature_engine.imputation import RandomSampleImputer\n",
        "\n",
        "imputer = RandomSampleImputer(random_state=42)\n",
        "\n",
        "# Fit and transform\n",
        "imputer.fit(X_train)\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ],
      "metadata": {
        "id": "_ESCLT9OHJQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assumptions\n",
        "- Data is misssing at random\n",
        "- MIssing values are replaced with other values within the same distribution of the original variable\n",
        "\n",
        "#### Pros\n",
        "- Easy to implement\n",
        "- Can be integrated in production\n",
        "- It preserves distributions\n",
        "\n",
        "#### Cons\n",
        "- Randomness\n",
        "- If there are many nmissing values the relationships between imputed variables and other variables may be affected\n",
        "- Memory allocation in production due to need to store both original and imputed datsets during imputation"
      ],
      "metadata": {
        "id": "_So1v7lIHZCw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zrzvwRsRJN7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Categorical Variables\n",
        "\n",
        "### Classic techniques\n",
        "- One-hot encoding\n",
        "- Frequency encoding\n",
        "- Ordinal / Laber encoding\n",
        "\n",
        "### Monotonic relationships\n",
        "- Ordered label encoding\n",
        "- Mean encoding\n",
        "- Weight of Evidence\n",
        "- Probability Ratio\n",
        "\n",
        "### Other techniques\n",
        "- Rare encoding\n",
        "- Binary encoding\n",
        "- Decision Tree encoding"
      ],
      "metadata": {
        "id": "lOvdMTCzJtIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Other - Rare Label encoding (first encoder to apply)\n",
        "\n",
        "from feature_engine.encoding import RareLabelEncoder\n",
        "\n",
        "encoder = RareLabelEncoder()\n",
        "\n",
        "# Fit and transform\n",
        "encoder.fit(X_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)\n"
      ],
      "metadata": {
        "id": "eq5TVGvDcZWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classic techniques - One-hot encoder\n",
        "\n",
        "from feature_engine.encoding import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Fit and transform\n",
        "encoder.fit(X_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)"
      ],
      "metadata": {
        "id": "_5jxUz9eLI8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pros\n",
        "- Doesn't assume distributions\n",
        "- Retains all categorical variable information\n",
        "- Works very with linear models\n",
        "\n",
        "#### Cons\n",
        "- Expands the feature space\n",
        "- Doesn't add any extra information while encoding\n",
        "- Add sparsity\n",
        "- Possible dummy variables may be identical"
      ],
      "metadata": {
        "id": "kNNkr9QvMpop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classic techniques - Frequency encoding\n",
        "\n",
        "from feature_engine.encoding import CountFrequencyEncoder\n",
        "\n",
        "encoder = CountFrequencyEncoder(encoding_method='frequency')\n",
        "\n",
        "# Fit and transform\n",
        "encoder.fit(X_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)"
      ],
      "metadata": {
        "id": "adipem_xNtvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pros\n",
        "- Easy to implement\n",
        "- Feature space remains the same size\n",
        "- Work well with tree-based algorithms\n",
        "\n",
        "#### Cons\n",
        "- Limitations with linear model\n",
        "- Does not handle new categories\n",
        "- If 2 or more categories have the same count/frequency information can be lost"
      ],
      "metadata": {
        "id": "C-74KbFcOng1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classic techniques - Label encoding\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform\n",
        "encoder.fit(X_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)"
      ],
      "metadata": {
        "id": "1J9w-mpePiye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pros\n",
        "- Easy to implement\n",
        "- Feature space remains the same size\n",
        "- Work well with tree-based algorithms\n",
        "\n",
        "#### Cons\n",
        "- Limitations with linear model\n",
        "- Does not handle new categories\n",
        "- Doesn't add any extra valuable information while encoding\n",
        "- Creates an ordered relatiosnships between the categories"
      ],
      "metadata": {
        "id": "NjjAoEziU-53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Monotinic techniques - Ordered encoding\n",
        "\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "encoder = OrdinalEncoder(encoding_method='ordered')\n",
        "\n",
        "# Fit and transform\n",
        "encoder.fit(X_train, y_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)"
      ],
      "metadata": {
        "id": "JTGWHBaUVd5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pros\n",
        "- Easy to implement\n",
        "- Feature space remains the same size\n",
        "- Creates a monotinic relationship between with the target variable\n",
        "- Works very well for regression problems\n",
        "\n",
        "#### Cons\n",
        "- Can overfit models"
      ],
      "metadata": {
        "id": "xvQRH416XI5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Monotinic - Mean Encoder\n",
        "\n",
        "from feature_engine.encoding import MeanEncoder\n",
        "\n",
        "encoder = MeanEncoder()\n",
        "\n",
        "# Fit and transform\n",
        "encoder.fit(X_train, y_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)"
      ],
      "metadata": {
        "id": "At45ZUVnX4JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pros\n",
        "- Easy to implement\n",
        "- Feature space remains the same size\n",
        "- Creates a monotinic relationship between with the target variable\n",
        "- Works very well for regression problems\n",
        "\n",
        "#### Cons\n",
        "- Can overfit models\n",
        "- If 2 ot more categproes have the same mean as the target information and relationships can decrease"
      ],
      "metadata": {
        "id": "cnOXi7lNYkTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Monotinic - Weight of Evidence (binary classification only)\n",
        "\n",
        "from feature_engine.encoding import WoEEncoder\n",
        "\n",
        "encoder = WoEEncoder()\n",
        "\n",
        "# Fit and transform\n",
        "encoder.fit(X_train, y_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)"
      ],
      "metadata": {
        "id": "oQKYGFUZY9bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pros\n",
        "- Easy to implement\n",
        "- Feature space remains the same size\n",
        "- Creates a monotinic relationship between with the target variable\n",
        "- Orders the categories in a log scale\n",
        "- Works great for analysis as it is easy to compare the transformed variables to determine which one is a better predictor\n",
        "\n",
        "#### Cons\n",
        "- Can overfit models\n",
        "- Undefined when denominatror is 0"
      ],
      "metadata": {
        "id": "dcXXiRlcZqyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Monotonic - Probability Ratios encoding (binary classification only)\n",
        "\n",
        "from feature_engine.encoding import PRatioEncoder\n",
        "\n",
        "encoder = PRatioEncoder()\n",
        "\n",
        "# Fit and transform\n",
        "encoder.fit(X_train, y_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)"
      ],
      "metadata": {
        "id": "lWyOEClAabZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pros\n",
        "- Easy to implement\n",
        "- Feature space remains the same size\n",
        "- Creates a monotinic relationship between with the target variable\n",
        "- Works well with linear models (as every other monotinic approach)\n",
        "- Works great for analysis as it is easy to compare the transformed variables to determine which one is a better predictor\n",
        "\n",
        "#### Cons\n",
        "- Can overfit models\n",
        "- Undefined when denominatror is 0"
      ],
      "metadata": {
        "id": "GPQxS-zubjfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Other - Binary encoder\n",
        "\n",
        "from category_encoders import BinaryEncoder\n",
        "\n",
        "encoder = BinaryEncoder()\n",
        "\n",
        "# Fit and transform\n",
        "encoder.fit(X_train, y_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)"
      ],
      "metadata": {
        "id": "Su7XJvWsb8zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pros\n",
        "- Easy to implement\n",
        "- Feature space remains ALMOST the same size\n",
        "\n",
        "#### Cons\n",
        "- Difficult to intepret\n",
        "- Potential loss of information during encoding"
      ],
      "metadata": {
        "id": "kHTzmOo8eOyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Linear assumptions (Transformations)\n",
        "\n",
        "- Logarithmic (right skewness)\n",
        "- Square root (right skewness)\n",
        "- Reciprocal (both)\n",
        "- Exponential (power) (left skewness)\n",
        "- Box-Cox\n",
        "- Yeo-Johnson"
      ],
      "metadata": {
        "id": "M8jMjj1If2mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box-Con (exponential transformation with automatic search)\n",
        "\n",
        "from feature_engine.transformation import BoxCoxTransformer\n",
        "\n",
        "transformer = BoxCoxTransformer()\n",
        "\n",
        "# Fit and transform\n",
        "transformer.fit(X_train)\n",
        "X_train = transformer.transform(X_train)\n",
        "X_test = transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "e6EWMeZFevfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Yeo- Johhson (exponential transformation with automatic search including negative values)\n",
        "\n",
        "from feature_engine.transformation import YeoJohnsonTransformer\n",
        "\n",
        "transformer = YeoJohnsonTransformer()\n",
        "\n",
        "# Fit and transform\n",
        "transformer.fit(X_train)\n",
        "X_train = transformer.transform(X_train)\n",
        "X_test = transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "iw5mN9cjkIix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Feature scales\n",
        "\n",
        "- Mean normalization\n",
        "- Standardization\n",
        "- Robust Scaling\n",
        "- Min Max\n",
        "- Absolute max\n",
        "- Unit norm"
      ],
      "metadata": {
        "id": "x6MtMbumlh7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "# Fit and transform\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "VyyJwqSzn_rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust Scaler\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "\n",
        "\n",
        "# Fit and transform\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "r5cNh9saoTxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  MinMax Scaler\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 10))\n",
        "\n",
        "# Fit and transform\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "pYEOTNQdpIst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Unit Norm Scaler\n",
        "\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "scaler = Normalizer()\n",
        "\n",
        "# Fit and transform\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "vr-kQYqbpyPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eGEvujZDpwjk"
      }
    }
  ]
}