{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boleslawkol/BKPublicRepo/blob/main/E2E_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPgWqg1P6ZwH"
      },
      "source": [
        "# 1. Understand the business problem\n",
        "* Define objective\n",
        "* How will DS/ML's solution be used?\n",
        "* What solutions are there in place?\n",
        "* How will you measure performance?\n",
        "* Minimum performance required?\n",
        "\n",
        "**IMPORTANT NOTE:** This is the most important step of the DS/ML project lifecycle. Make sure you define performance metrics and targets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF1T7F7R6Z8G"
      },
      "source": [
        "# 2. Prepare your DS/ML environment\n",
        "\n",
        "*Why is this step relevant?*\n",
        "\n",
        "It minimizes the risk of libary conflicts when sharing your project.\n",
        "\n",
        "https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVw93nwW7m2j"
      },
      "source": [
        "# Using conda\n",
        "# The use of ! will allow you to run terminal commands from your notebook\n",
        "\n",
        "# Option 1: Create new environment from scratch\n",
        "# NOTE: ipykernel is used only to select kernel in jupyter lab or jupyter notebook\n",
        "!conda create -y -n envname python ipykernel\n",
        "!conda activate envname\n",
        "\n",
        "# Create environment yaml file\n",
        "!conda env export > filename.yaml\n",
        "\n",
        "# Option 2: Create environment from yaml file\n",
        "!conda create -f flename.yaml\n",
        "\n",
        "# Option 3: Update an existing enviromment from a yaml file\n",
        "!conda env update envname -f filename.yaml\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgEUjokr6aCA"
      },
      "source": [
        "# 3. Get data\n",
        "\n",
        "* Find data and document sources\n",
        "* Check for space\n",
        ">* If RAM > datafile size, then do batch training\n",
        ">* If RAM < datafile size, sample or do online training\n",
        "\n",
        "* Check  terms and conditions\n",
        "* If applicable: get access\n",
        "* Create workspace / define storage location\n",
        "* Get data\n",
        "* Deal with sensitive information (delete, protect, anonymize)\n",
        "* Sample test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ359bDi-7JQ"
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Visit https://ipython.readthedocs.io/en/stable/interactive/plotting.html for information on %matplotlib\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Hide warnings if you are presenting your project to an audience to make your code look cleaner\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Visit https://docs.python.org/3/library/warnings.html for information on warning control"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1y2YkxgA-QS"
      },
      "source": [
        "# Option 1a: Import data from csv\n",
        "df = pd.read_csv(\"filename.csv\")\n",
        "\n",
        "# Option 1b: Import data from excel\n",
        "df = pd.read_excel(\"filename.xlsx\")\n",
        "\n",
        "# Option 2: Import data from database\n",
        "import pyodbc\n",
        "\n",
        "# Define connector to database\n",
        "driver = 'SQL Server'\n",
        "server = 'servername or ip address'\n",
        "database = 'mydb'\n",
        "username = 'myusername'\n",
        "password = 'mypassword'\n",
        "table = 'mytable'\n",
        "\n",
        "connector = pyodbc.connect('DRIVER={'+driver+'};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+password)\n",
        "\n",
        "# Define query\n",
        "query = 'SELECT * FROM '+table\n",
        "\n",
        "# Import query results into DataFrame\n",
        "df = pd.read_sql(query, connector)\n",
        "\n",
        "# Visit https://github.com/mkleehammer/pyodbc/wiki to learn more about pyobdc and its available drivers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Jlglmj6aEm"
      },
      "source": [
        "# 4. Explore data to gain insights\n",
        "\n",
        "Look for:\n",
        "* Target variable (for supervised learning)\n",
        "* Is your dataset imbalanced? (for classification)\n",
        "* Size of dataset (to ensure if fits in memory)\n",
        "* Number of variables (to estimate model complexity and processing)\n",
        "* Variable data types (important for choosing visualizations https://python-graph-gallery.com/ and transformations https://feature-engine.trainindata.com/)\n",
        "* Missing values\n",
        "* Cardinality (categorical variables)\n",
        "* Rare values (categorical variables)\n",
        "* Distributions (skewness/kurtosis)\n",
        "* Scale\n",
        "* Outliers\n",
        "* Correlations (pay particular attention in correlations with target variable)\n",
        "* Hypothesis testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DkQ2I8OGUk1"
      },
      "source": [
        "## Size, variables and dtypes\n",
        "df.info(memory_usage='deep')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfQkHLSNGkMX"
      },
      "source": [
        "# Missing values\n",
        "df.isnull().mean().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ugcCwIGu2W"
      },
      "source": [
        "# Cardinality\n",
        "df.describe(include='O')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVe__Mk4JhBx"
      },
      "source": [
        "When performing feature selection, you can feel safe to remove variables that meet the following conditions:\n",
        "* Cardinality >= 95% of observations\n",
        "* Cardinality == 1 (this means you have a constant categorical *variable*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6oDHy9gG-QJ"
      },
      "source": [
        "# Rare values (for categorical ONLY)\n",
        "df.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnZnAgKoHa-s"
      },
      "source": [
        "# Distributions, scale, and outliers\n",
        "df.describe()\n",
        "# For distributions, look for differences in mean vs std vs percentiles\n",
        "# For scale, look for different orders of magnitud between variables' min/max\n",
        "\n",
        "# Distributions only (for outliers, values further away from zero indicate more extreme outliers)\n",
        "df.skew().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U8oAYNSIKix"
      },
      "source": [
        "# Numerical Correlations\n",
        "df.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Correlations\n",
        "# V-Cramer's"
      ],
      "metadata": {
        "id": "-doDaZR9tj_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjN6VQSP6aHc"
      },
      "source": [
        "# 5. Prepare your data (*create transformers' pipeline*)\n",
        "\n",
        "## *Important notes:*\n",
        "1. Split your dataset in train and test before applying any fit method.\n",
        "2. fit ONLY training data\n",
        "3. Transform both train and test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e69KW2yPMsW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(columns=['Target'])\n",
        "y = df['Target']\n",
        "\n",
        "# For the larger the dataset, the smaller the test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsWJGvTKezlD"
      },
      "source": [
        "## 5.1 Feature engineering\n",
        "* Missing values\n",
        "* Label encoding\n",
        "* Outlier handling\n",
        "* Scaling\n",
        "* Dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD2PhiQ9fres"
      },
      "source": [
        "Handling missing values:\n",
        "https://feature-engine.readthedocs.io/en/latest/imputation/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkXrPHktfGll"
      },
      "source": [
        "# Numerical missing values. Use median for skewed data and mean for gaussian distributions\n",
        "\n",
        "from feature_engine.imputation import MeanMedianImputer\n",
        "num_imputer = MeanMedianImputer()\n",
        "\n",
        "# Categorical missing values will replace NaN with Missing\n",
        "from feature_engine.imputation import CategoricalImputer\n",
        "cat_imputer = CategoricalImputer(value='Missing')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOn9zz0uf1Wx"
      },
      "source": [
        "Categorical encoders: https://feature-engine.readthedocs.io/en/latest/encoding/index.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UBJX9fLf-pm"
      },
      "source": [
        "# Rare label encoder for high cardinality variables where there may be elements in test but not in train. Will replace values with Rare\n",
        "from feature_engine.encoding import RareLabelEncoder\n",
        "rare_encoder = RareLabelEncoder(replace_with='Rare')\n",
        "\n",
        "# Select the best encoder based on your use case and dataset. If you are unsure, try OrdinalEncoder\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "cat_encoder = OrdinalEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2aRyNj4hXzX"
      },
      "source": [
        "Handling outliers: https://feature-engine.readthedocs.io/en/latest/outliers/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAltp3NrhUEU"
      },
      "source": [
        "# Winsorizer() caps maximum and / or minimum values of a variable\n",
        "\n",
        "from feature_engine.outliers import Winsorizer\n",
        "capper = Winsorizer(capping_method='iqr', tail='both')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrWrUvj3ioMx"
      },
      "source": [
        "Data scaling: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zwg0U-Oi1we"
      },
      "source": [
        "# The most popular scalers are StandardScaler and RobustScaler. More info in the link above\n",
        "\n",
        "# Recommended use of sklearn wrapper to continue working with DataFrames\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from feature_engine.wrappers import SklearnTransformerWrapper\n",
        "scaler = SklearnTransformerWrapper(transformer = RobustScaler())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViO-uV40kAXI"
      },
      "source": [
        "Dimensionality reduction with PCA: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Q9zZNskKjy"
      },
      "source": [
        "# Use n_components < 1 for explained variance sum and n_components > 1 for specific number of components to keep\n",
        "# Besides PCA you can also try LDA, tSNE, IsoMap, etc.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "dim_red = SklearnTransformerWrapper(transformer = PCA(n_components=0.95))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTEzusAM6aKG"
      },
      "source": [
        "## 5.2 Feature selection\n",
        "https://scikit-learn.org/stable/modules/feature_selection.html\n",
        "\n",
        "https://feature-engine.readthedocs.io/en/latest/selection/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LLEXQ3qNi8s"
      },
      "source": [
        "# Basic feature selection: remove constant, quasi-constant and duplicate variables\n",
        "\n",
        "from feature_engine.selection import DropConstantFeatures\n",
        "cons_features = DropConstantFeatures(tol=0.95)\n",
        "\n",
        "from feature_engine.selection import DropDuplicateFeatures\n",
        "duplicates = DropDuplicateFeatures()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlvBuDRMlX1e"
      },
      "source": [
        "# Correlated feature selection (colinearity)\n",
        "\n",
        "# Change scoring, selection_method, and estimator when applicable\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "corr_features = SmartCorrelatedSelection(selection_method=\"variance\",estimator=None, scoring='roc_auc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyYeIFjBoPxC"
      },
      "source": [
        "For wrapper algorithms (Forward selection, backward selection, exhaustive search) refer to:\n",
        "* Forward/backward selection: http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/\n",
        "* Exhaustive search: http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/\n",
        "\n",
        "*NOTE: Wrapper methods are computationally greedy and require a model to be tried on.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do3OZHQLq-7d"
      },
      "source": [
        "# Embedded feature selection with sklearn.feature_selection.SelectFromModel\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZWF5k4jsdTs"
      },
      "source": [
        "Hybrid feature selection techniques:\n",
        "* **SelectByShuffling**: selects features by determining the drop in machine learning model performance when each feature’s values are randomly shuffled\n",
        "* **RecursiveFeatureElimination** and RecursiveFeatureAddition: selects features following a recursive process\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ1wF03qsZpk"
      },
      "source": [
        "# Shuffle selector depends on a model to train\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "\n",
        "from feature_engine.selection import ShuffleFeaturesSelector\n",
        "shuffle_selector = SelectByShuffling(estimator=model, scoring=\"r2\", cv=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpef-2nvzr5R"
      },
      "source": [
        "# RecursiveFeatureElimination depends on a model to train\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "\n",
        "from feature_engine.selection import RecursiveFeatureElimination\n",
        "recursive_selector = RecursiveFeatureElimination(estimator=model, scoring=\"r2\", cv=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMu8BA-d0EMB"
      },
      "source": [
        "## 5.3 Create your transformer pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fcgj9EMz23G"
      },
      "source": [
        "# Import make_pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Define pipeline steps\n",
        "transformer_pipe = make_pipeline(num_imputer,\n",
        "                                 cat_imputer,\n",
        "                                 rare_encoder,\n",
        "                                 cat_encoder,\n",
        "                                 capper,\n",
        "                                 scaler,\n",
        "                                 cons_features,\n",
        "                                 duplicates,\n",
        "                                 corr_features,\n",
        "                                 recursive_selector,\n",
        "                                 dim_red,\n",
        "                                 )\n",
        "\n",
        "#fit and transform\n",
        "X_train = transformer_pipe.fit_transform(X_train, y_train)\n",
        "X_test = transformer_pipe.transform(X_test, y_test)\n",
        "\n",
        "# Save pipeline in pickle file\n",
        "import pickle\n",
        "\n",
        "with open ('transformer_pipeline.pkl', 'wb') as name:\n",
        "  pickle.dump(transformer_pipe, name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UXdeI_26aM3"
      },
      "source": [
        "# 6. Try different algorithms\n",
        "(https://mlflow.org/ can be used to track models’ performance and hyper parameters)\n",
        "* Try default hyperparameters first (unless you have good notion on what hyperparams will improve performance and their values)\n",
        "* Measure and compare performance\n",
        "* Short-list top 1-3 models for hyperparameter tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uguwelDm2XMK"
      },
      "source": [
        "# Import algorithms to try\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from tpot import TPOTRegressor\n",
        "from catboost import CatBoostRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WARCZuq2iI5"
      },
      "source": [
        "# Create table with models and its performance\n",
        "# IMPORTANT NOTE: change metrics and algorithms as you see fit\n",
        "\n",
        "# Define algorithms to try\n",
        "algorithms = []\n",
        "algorithms.append(('ElasticNet', ElasticNet()))\n",
        "algorithms.append(('Random Forest', RandomForestRegressor()))\n",
        "algorithms.append(('XG Boost', XGBRegressor()))\n",
        "algorithms.append(('LightGBM', LGBMRegressor()))\n",
        "algorithms.append(('Genetic', TPOTRegressor()))\n",
        "algorithms.append(('CatBoost', CatBoostRegressor()))\n",
        "\n",
        "# Initialize lists\n",
        "names = []\n",
        "train_rmse = []\n",
        "test_rmse = []\n",
        "train_r2 = []\n",
        "test_r2 = []\n",
        "cross_validation_scores = []\n",
        "runtime = []\n",
        "\n",
        "# Iterate ofer each element in algorithms\n",
        "for name, reg in algorithms:\n",
        "  t1 = time.time()\n",
        "  names.append(name)\n",
        "  reg.fit(X_train, y_train)\n",
        "  train_rmse.append(mean_squared_error(y_train, reg.predict(X_train), squared=False))\n",
        "  test_rmse.append(mean_squared_error(y_test, reg.predict(X_test), squared=False))\n",
        "  train_r2.append(r2_score(y_train, reg.predict(X_train)))\n",
        "  test_r2.append(r2_score(y_test, reg.predict(X_test)))\n",
        "  cross_validation_scores.append(cross_val_score(reg, np.vstack((X_train, X_test)), np.hstack((y_train, y_test))).mean())\n",
        "  t2 = time.time()\n",
        "  runtime.append(t2-t1)\n",
        "\n",
        "# Create comparison dataframe\n",
        "model_comparison = pd.DataFrame({'Algorithm': names,\n",
        "                                 'Train RMSE': train_rmse,\n",
        "                                 'Test_RMSE': test_rmse,\n",
        "                                 'Train r2 score': train_r2,\n",
        "                                 'Test r2 score': test_r2,\n",
        "                                 'Cross validation score': cross_validation_scores,\n",
        "                                 'Runtime': runtime,})\n",
        "\n",
        "# Sort by Test_RMSE\n",
        "model_comparison.sort_values(by=['Test_RMSE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model explainability\n",
        "\n",
        "- SHAP\n",
        "- LIME"
      ],
      "metadata": {
        "id": "URLjwH4P3Dt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Deployment and tracking (MLOps)\n",
        "\n",
        "- Deployment (Flask, Django, GCP, AWS, Azure, Databricks etc.)\n",
        "- Tracking (W&B, MLFlow)"
      ],
      "metadata": {
        "id": "zqXmjPBN3Hol"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_5OnjKe6aPz"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbkS8cTz6aSp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaqL1csJ6aVX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGRgMQIH6aYD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csMoHP1t6aai"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1uEzYx56adM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0WmsJvMUGvs"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}