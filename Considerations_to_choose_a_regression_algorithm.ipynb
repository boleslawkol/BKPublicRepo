{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boleslawkol/Purdue-Notebooks/blob/main/Considerations_to_choose_a_regression_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "| Algorithm | Data Size Suitability | Type of Data | Computational Resource Intensity | Model Complexity | Interpretability | Mathematical Considerations in EDA | Data Considerations | Strengths | Weaknesses | Additional Notes |\n",
        "|-----------|-----------------------|--------------|----------------------------------|------------------|------------------|------------------------------------|---------------------|-----------|------------|------------------|\n",
        "| Linear Regression | Small to Medium | Numeric, Continuous | Low | Low | High | Linear relationships, sensitive to outliers and multicollinearity, assumes normal distribution, scale-sensitive | Fast, low memory | Simple, interpretable, good baseline | Sensitive to outliers, can't capture non-linear patterns | Good for large datasets, fast, interpretable |\n",
        "| Ridge Regression | Small to Medium | Numeric, Continuous | Low | Medium | High | Linear relationships, L2 regularization, robust to multicollinearity and scale, assumes normal distribution | Fast, low memory | Handles multicollinearity, prevents overfitting | Not ideal for highly non-linear data | Good for large datasets, regularized to prevent overfitting |\n",
        "| Lasso Regression | Small to Medium | Numeric, Continuous | Low | Medium | High | Linear relationships, L1 regularization (feature selection), sensitive to scale, assumes normal distribution | Fast, low memory | Performs feature selection, good for sparse data | Sensitive to outliers, can produce biased results | Good for feature selection and sparse data |\n",
        "| Polynomial Regression | Small to Medium | Numeric, Continuous | Medium | High | Medium | Non-linear relationships, sensitive to outliers and scale, can lead to overfitting, assumes normal distribution | Moderate speed, moderate memory | Can model non-linear patterns | Can overfit, sensitive to feature scaling | Handles non-linear relationships well |\n",
        "| Support Vector Regression (SVR) | Small to Medium | Numeric, Continuous | High | High | Low | Handles outliers, good for non-linear and high-dimensional data, kernel-dependent, sensitive to scale | Moderate speed, moderate memory | Works well with high-dimensional data, robust to outliers | Kernel choice and hyperparameter tuning can be challenging | Well-suited for outliers and high-dimensional data |\n",
        "| Decision Tree Regression | Small to Medium | Numeric, Categorical | Medium | High | High | Handles non-linear data and mixed data types, robust to outliers and scale, prone to overfitting | Moderate speed, moderate memory | Interpretable, handles mixed data types | Prone to overfitting, less effective with very high-dimensional data | Good for mixed data types, but can overfit |\n",
        "| Random Forest Regression | Small to Large | Numeric, Categorical | High | High | Medium | Ensemble method, handles non-linearity and mixed data types, robust to outliers, less sensitive to scale, less prone to overfitting | Moderate speed, high memory | Robust to overfitting, good performance on various datasets | Less interpretable than single trees, can be computationally expensive | Combines multiple algorithms for improved performance |\n",
        "| K-Nearest Neighbors (KNN) | Small to Medium | Numeric, Categorical | High | Low | Low | No assumptions about data distribution, very sensitive to scale, performance degrades with high number of features | Slow for large datasets, moderate memory | Simple, handles high-dimensional data well, robust to outliers | Sensitive to the choice of k and distance metric, not good at capturing global trends | Good for high-dimensional data but sensitive to k and distance metric |\n",
        "\n",
        "**Additional Considerations:**\n",
        "\n",
        "- **Distribution:** Algorithms like Linear Regression, Ridge, and Lasso assume normal distribution of features. Non-parametric methods like Decision Trees and KNN do not make such assumptions.\n",
        "- **Scale:** Scale sensitivity is crucial for algorithms like SVR and KNN, where feature scaling improves performance. Regularization methods (Ridge, Lasso) are also sensitive to scale.\n",
        "- **Number of Features:** High-dimensional data can be better handled by methods like Random Forest or SVR, while algorithms like KNN can suffer in performance.\n",
        "- **Data Type:** Mixed data types (numeric and categorical) are well handled by tree-based methods like Decision Trees and Random Forests.\n",
        "\n",
        "**Best Practices:**\n",
        "\n",
        "- Experiment with different algorithms to find the best fit for your specific dataset and problem.\n",
        "- Evaluate model performance using appropriate metrics (e.g., R-squared, mean squared error).\n",
        "- Consider cross-validation to prevent overfitting.\n",
        "- Use feature scaling for algorithms sensitive to feature ranges.\n",
        "- Explore hyperparameter tuning to optimize model performance."
      ],
      "metadata": {
        "id": "FoCC0sYDOQFq"
      }
    }
  ]
}